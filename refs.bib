@article{Golub2016,
  title = {Matrix {{Computation}}},
  author = {Golub, Gene H.},
  editor = {{Intergovernmental Panel on Climate Change}},
  year = {2016},
  volume = {25},
  number = {3},
  eprint = {1011.1669v3},
  pages = {228--234},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  issn = {10623264},
  doi = {10.4037/ajcc2016979},
  abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein-protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-{$\alpha$}-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD {$\leq$} 2.0 {\AA} for the interface backbone atoms) increased from 21\% with default Glide SP settings to 58\% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63\% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40\% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
  archiveprefix = {arXiv},
  isbn = {9788578110796},
  pmid = {25246403},
  keywords = {Mobile,Named entity disambiguation,Natural language processing,News,Recommender system},
  file = {/Users/liujinguo/Zotero/storage/5AZ5KKJF/Golub_2016_Matrix Computation.pdf}
}

@incollection{Alman2021,
  title = {A {{Refined Laser Method}} and {{Faster Matrix Multiplication}}},
  booktitle = {Proceedings of the 2021 {{ACM-SIAM Symposium}} on {{Discrete Algorithms}} ({{SODA}})},
  author = {Alman, Josh and Williams, Virginia Vassilevska},
  year = {2021},
  month = jan,
  series = {Proceedings},
  pages = {522--539},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611976465.32},
  urldate = {2024-04-29},
  abstract = {The complexity of matrix multiplication is measured in terms of {$\omega$}, the smallest real number such that two n {\texttimes} n matrices can be multiplied using O(n{$\omega$}+∊) field operations for all ∊ {$>$} 0; the best bound until now is {$\omega$} {$<$} 2.37287 [Le Gall'14]. All bounds on {$\omega$} since 1986 have been obtained using the so-called laser method, a way to lower-bound the `value' of a tensor in designing matrix multiplication algorithms. The main result of this paper is a refinement of the laser method that improves the resulting value bound for most sufficiently large tensors. Thus, even before computing any specific values, it is clear that we achieve an improved bound on {$\omega$}, and we indeed obtain the best bound on {$\omega$} to date: {$\omega$} {$<$} 2.37286. The improvement is of the same magnitude as the improvement that [Le Gall'14] obtained over the previous bound [Vassilevska W.'12]. Our improvement to the laser method is quite general, and we believe it will have further applications in arithmetic complexity.},
  file = {/Users/liujinguo/Zotero/storage/4I9ZCACP/Alman and Williams - 2021 - A Refined Laser Method and Faster Matrix Multiplic.pdf}
}

@misc{Liu2022,
  doi = {10.48550/ARXIV.2205.03718},
  url = {https://arxiv.org/abs/2205.03718},
  author = {Liu, Jin-Guo and Gao, Xun and Cain, Madelyn and Lukin, Mikhail D. and Wang, Sheng-Tao},
  keywords = {Statistical Mechanics (cond-mat.stat-mech), FOS: Physical sciences, FOS: Physical sciences},
  title = {Computing solution space properties of combinatorial optimization problems via generic tensor networks},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}
@misc{Pan2021,
      title={Simulating the Sycamore quantum supremacy circuits}, 
      author={Feng Pan and Pan Zhang},
      year={2021},
      eprint={2103.03074},
      archivePrefix={arXiv},
      primaryClass={quant-ph}
}
@Article{Stefanos2019,
	title={{Fast counting with tensor networks}},
	author={Stefanos Kourtis and Claudio Chamon and Eduardo R. Mucciolo and Andrei E. Ruckenstein},
	journal={SciPost Phys.},
	volume={7},
	issue={5},
	pages={60},
	year={2019},
	publisher={SciPost},
	doi={10.21468/SciPostPhys.7.5.060},
	url={https://scipost.org/10.21468/SciPostPhys.7.5.060},
}
@article{Gray2021,
   title={Hyper-optimized tensor network contraction},
   volume={5},
   ISSN={2521-327X},
   url={http://dx.doi.org/10.22331/q-2021-03-15-410},
   DOI={10.22331/q-2021-03-15-410},
   journal={Quantum},
   publisher={Verein zur Forderung des Open Access Publizierens in den Quantenwissenschaften},
   author={Gray, Johnnie and Kourtis, Stefanos},
   year={2021},
   month={Mar},
   pages={410}
}
@misc{Kalachev2021,
      title={Recursive Multi-Tensor Contraction for XEB Verification of Quantum Circuits}, 
      author={Gleb Kalachev and Pavel Panteleev and Man-Hong Yung},
      year={2021},
      eprint={2108.05665},
      archivePrefix={arXiv},
      primaryClass={quant-ph}
}

@article{Roa2024,
  title={Probabilistic Inference in the Era of Tensor Networks and Differential Programming},
  author={Roa-Villescas, Martin and Gao, Xuanzhao and Stuijk, Sander and Corporaal, Henk and Liu, Jin-Guo},
  journal={arXiv preprint arXiv:2405.14060},
  year={2024},
  url={https://arxiv.org/abs/2405.14060}
}

@book{Strang2022,
  title={Introduction to linear algebra},
  author={Strang, Gilbert},
  year={2022},
  publisher={SIAM}
}

@book{Golub2013,
  title={Matrix computations},
  author={Golub, Gene H and Van Loan, Charles F},
  volume={3},
  year={2013},
  publisher={JHU press},
  doi={10.2307/3621013}
}

@article{Cichocki2014,
  title={Era of big data processing: A new approach via tensor networks and tensor decompositions},
  author={Cichocki, Andrzej},
  journal={arXiv preprint arXiv:1403.2048},
  year={2014},
  url={https://arxiv.org/abs/1403.2048}
}

@article{Ng2001,
  title={On spectral clustering: Analysis and an algorithm},
  author={Ng, Andrew and Jordan, Michael and Weiss, Yair},
  journal={Advances in neural information processing systems},
  volume={14},
  year={2001}
}

@book{Bishop2006,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M and Nasrabadi, Nasser M},
  volume={4},
  number={4},
  year={2006},
  publisher={Springer}
}

@article{Cooley1965,
  title={An algorithm for the machine calculation of complex Fourier series},
  author={Cooley, James W and Tukey, John W},
  journal={Mathematics of computation},
  volume={19},
  number={90},
  pages={297--301},
  year={1965},
  publisher={JSTOR}
}

@article{Alman2024,
  title = {A {{Refined Laser Method}} and {{Faster Matrix Multiplication}}},
  author = {Alman, Josh and Williams, Virginia Vassilevska},
  year = {2024},
  month = sep,
  journal = {TheoretiCS},
  volume = {Volume 3},
  publisher = {Episciences.org},
  issn = {2751-4838},
  doi = {10.46298/theoretics.24.21},
  urldate = {2025-03-09},
  abstract = {The complexity of matrix multiplication is measured in terms of \${\textbackslash}omega\$, the smallest real number such that two \$n{\textbackslash}times n\$ matrices can be multiplied using \$O(n{\textasciicircum}\{{\textbackslash}omega+{\textbackslash}epsilon\})\$ field operations for all \${\textbackslash}epsilon{$>$}0\$; the best bound until now is \${\textbackslash}omega{$<$}2.37287\$ [Le Gall'14]. All bounds on \${\textbackslash}omega\$ since 1986 have been obtained using the so-called laser method, a way to lower-bound the `value' of a tensor in designing matrix multiplication algorithms. The main result of this paper is a refinement of the laser method that improves the resulting value bound for most sufficiently large tensors. Thus, even before computing any specific values, it is clear that we achieve an improved bound on \${\textbackslash}omega\$, and we indeed obtain the best bound on \${\textbackslash}omega\$ to date: \$\${\textbackslash}omega {$<$} 2.37286.\$\$ The improvement is of the same magnitude as the improvement that [Le Gall'14] obtained over the previous bound [Vassilevska W.'12]. Our improvement to the laser method is quite general, and we believe it will have further applications in arithmetic complexity.},
  file = {/home/leo/snap/zotero-snap/common/Zotero/storage/Y4YVKXBU/Alman and Williams - 2024 - A Refined Laser Method and Faster Matrix Multiplication.pdf}
}

@article{Strassen1969,
  title={Gaussian elimination is not optimal},
  author={Strassen, Volker},
  journal={Numerische mathematik},
  volume={13},
  number={4},
  pages={354--356},
  year={1969},
  publisher={Springer}
}

@book{Moore2011,
  author = {Moore, Cristopher and Mertens, Stephan},
  title = {The Nature of Computation},
  year = {2011},
  isbn = {0199233217},
  publisher = {Oxford University Press, Inc.},
  address = {USA},
  abstract = {Computational complexity is one of the most beautiful fields of modern mathematics, and it is increasingly relevant to other sciences ranging from physics to biology. But this beauty is often buried underneath layers of unnecessary formalism, and exciting recent results like interactive proofs, cryptography, and quantum computing are usually considered too "advanced" to show to the typical student. The aim of this book is to bridge both gaps by explaining the deep ideas of theoretical computer science in a clear and enjoyable fashion, making them accessible to non computer scientists and to computer scientists who finally want to understand what their formalisms are actually telling. This book gives a lucid and playful explanation of the field, starting with P and NP-completeness. The authors explain why the P vs. NP problem is so fundamental, and why it is so hard to resolve. They then lead the reader through the complexity of mazes and games; optimization in theory and practice; randomized algorithms, interactive proofs, and pseudorandomness; Markov chains and phase transitions; and the outer reaches of quantum computing. At every turn, they use a minimum of formalism, providing explanations that are both deep and accessible. The book is intended for graduates and undergraduates, scientists from other areas who have long wanted to understand this subject, and experts who want to fall in love with this field all over again.}
}

@misc{Sandryhaila2013,
  title = {Eigendecomposition of {{Block Tridiagonal Matrices}}},
  author = {Sandryhaila, Aliaksei and Moura, Jose M. F.},
  year = {2013},
  month = jun,
  number = {arXiv:1306.0217},
  eprint = {1306.0217},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1306.0217},
  urldate = {2025-03-15},
  abstract = {Block tridiagonal matrices arise in applied mathematics, physics, and signal processing. Many applications require knowledge of eigenvalues and eigenvectors of block tridiagonal matrices, which can be prohibitively expensive for large matrix sizes. In this paper, we address the problem of the eigendecomposition of block tridiagonal matrices by studying a connection between their eigenvalues and zeros of appropriate matrix polynomials. We use this connection with matrix polynomials to derive a closed-form expression for the eigenvectors of block tridiagonal matrices, which eliminates the need for their direct calculation and can lead to a faster calculation of eigenvalues. We also demonstrate with an example that our work can lead to fast algorithms for the eigenvector expansion for block tridiagonal matrices.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Spectral Theory},
  file = {/home/leo/snap/zotero-snap/common/Zotero/storage/RX93P67A/Sandryhaila and Moura - 2013 - Eigendecomposition of Block Tridiagonal Matrices.pdf;/home/leo/snap/zotero-snap/common/Zotero/storage/HILDF7H5/1306.html}
}

@article{Molinari2008,
  title = {Determinants of {{Block Tridiagonal Matrices}}},
  author = {Molinari, Luca G.},
  year = {2008},
  month = oct,
  journal = {Linear Algebra and its Applications},
  volume = {429},
  number = {8-9},
  eprint = {0712.0681},
  primaryclass = {math-ph},
  pages = {2221--2226},
  issn = {00243795},
  doi = {10.1016/j.laa.2008.06.015},
  urldate = {2025-03-15},
  abstract = {An identity is proven that evaluates the determinant of a block tridiagonal matrix with (or without) corners as the determinant of the associated transfer matrix (or a submatrix of it).},
  archiveprefix = {arXiv},
  keywords = {Mathematical Physics,Mathematics - Mathematical Physics,Mathematics - Numerical Analysis},
  file = {/home/leo/snap/zotero-snap/common/Zotero/storage/TEDGJ3SC/Molinari - 2008 - Determinants of Block Tridiagonal Matrices.pdf;/home/leo/snap/zotero-snap/common/Zotero/storage/ZUDY94C8/0712.html}
}

@article{Ran2006,
  title = {The Inverses of Block Tridiagonal Matrices},
  author = {Ran, Rui-Sheng and Huang, Ting-Zhu},
  year = {2006},
  month = aug,
  journal = {Applied Mathematics and Computation},
  volume = {179},
  number = {1},
  pages = {243--247},
  issn = {0096-3003},
  doi = {10.1016/j.amc.2005.11.098},
  urldate = {2025-03-15},
  abstract = {Firstly, the twisted block decompositions of the block tridiagonal matrices are presented. According to the special structure of the decomposition, the formulae of computing the block elements of each column of the inverse matrices are obtained. Then an algorithm of inverting the block tridiagonal matrices has been established. The explicit expressions of the block elements of the inverse matrices are also presented. At last, for the algorithm in this paper and some existed algorithms for the inverse matrices, the calculating complexity and the calculating time have been compared.},
  keywords = {Algorithm,Block tridiagonal matrix,The inverse matrices,The twisted block decomposition},
  file = {/home/leo/snap/zotero-snap/common/Zotero/storage/B6BV7QND/S0096300305010337.html}
}

@article{Dhillon2004,
  title = {Multiple Representations to Compute Orthogonal Eigenvectors of Symmetric Tridiagonal Matrices},
  author = {Dhillon, Inderjit S. and Parlett, Beresford N.},
  year = {2004},
  month = aug,
  journal = {Linear Algebra and its Applications},
  volume = {387},
  pages = {1--28},
  issn = {0024-3795},
  doi = {10.1016/j.laa.2003.12.028},
  urldate = {2025-03-15},
  abstract = {In this paper we present an O(nk) procedure, Algorithm MR3, for computing k eigenvectors of an n{\texttimes}n symmetric tridiagonal matrix T. A salient feature of the algorithm is that a number of different LDLt products (L unit lower triangular, D diagonal) are computed. In exact arithmetic each LDLt is a factorization of a translate of T. We call the various LDLt products representations (of T) and, roughly speaking, there is a representation for each cluster of close eigenvalues. The unfolding of the algorithm, for each matrix, is well described by a representation tree. We present the tree and use it to show that if each representation satisfies three prescribed conditions then the computed eigenvectors are orthogonal to working accuracy and have small residual norms with respect to the original matrix T.},
  keywords = {Eigenvectors,High relative accuracy,Orthogonality,Relatively robust representations (RRR),Symmetric tridiagonal},
  file = {/home/leo/snap/zotero-snap/common/Zotero/storage/2JIYQI7Q/Dhillon and Parlett - 2004 - Multiple representations to compute orthogonal eigenvectors of symmetric tridiagonal matrices.pdf;/home/leo/snap/zotero-snap/common/Zotero/storage/TNPDAVFQ/S002437950300908X.html}
}

@article{Markov2008,
  title = {Simulating {{Quantum Computation}} by {{Contracting Tensor Networks}}},
  author = {Markov, Igor L. and Shi, Yaoyun},
  year = {2008},
  month = jan,
  journal = {SIAM Journal on Computing},
  volume = {38},
  number = {3},
  pages = {963--981},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0097-5397},
  doi = {10.1137/050644756},
  urldate = {2023-10-05},
  abstract = {Adiabatic quantum computation has recently attracted attention in the physics and computer science communities, but its computational power was unknown. We describe an efficient adiabatic simulation of any given quantum algorithm, which implies that the adiabatic computation model and the conventional quantum computation model are polynomially equivalent. Our result can be extended to the physically realistic setting of particles arranged on a two-dimensional grid with nearest neighbor interactions. The equivalence between the models allows stating the main open problems in quantum computation using well-studied mathematical objects such as eigenvectors and spectral gaps of sparse matrices.},
  file = {/Users/liujinguo/Zotero/storage/SKT7RY29/Markov_Shi_2008_Simulating Quantum Computation by Contracting Tensor Networks.pdf}
}

@article{Liu2021,
  title = {Tropical {{Tensor Network}} for {{Ground States}} of {{Spin Glasses}}},
  author = {Liu, Jin-Guo and Wang, Lei and Zhang, Pan},
  year = {2021},
  month = mar,
  journal = {Physical Review Letters},
  volume = {126},
  number = {9},
  pages = {090506},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.126.090506},
  urldate = {2023-03-23},
  abstract = {We present a unified exact tensor network approach to compute the ground state energy, identify the optimal configuration, and count the number of solutions for spin glasses. The method is based on tensor networks with the tropical algebra defined on the semiring of (R{$\cup$}\{-{$\infty$}\},{$\oplus$},{$\odot$}). Contracting the tropical tensor network gives the ground state energy; differentiating through the tensor network contraction gives the ground state configuration; mixing the tropical algebra and the ordinary algebra counts the ground state degeneracy. The approach brings together the concepts from graphical models, tensor networks, differentiable programming, and quantum circuit simulation, and easily utilizes the computational power of graphical processing units (GPUs). For applications, we compute the exact ground state energy of Ising spin glasses on square lattice up to 1024 spins, on cubic lattice up to 216 spins, and on three regular random graphs up to 220 spins, on a single GPU; we obtain exact ground state energy of {\textpm}J Ising spin glass on the chimera graph of D-Wave quantum annealer of 512 qubits in less than 100 s and investigate the exact value of the residual entropy of {\textpm}J spin glasses on the chimera graph; finally, we investigate ground-state energy and entropy of three-state Potts glasses on square lattices up to size 18{\texttimes}18. Our approach provides baselines and benchmarks for exact algorithms for spin glasses and combinatorial optimization problems, and for evaluating heuristic algorithms and mean-field theories.},
  file = {/home/leo/snap/zotero-snap/common/Zotero/storage/IM6JTJMW/Liu et al. - 2021 - Tropical Tensor Network for Ground States of Spin .pdf}
}

@misc{Qing2024,
  title = {Compressing Neural Network by Tensor Network with Exponentially Fewer Variational Parameters},
  author = {Qing, Yong and Li, Ke and Zhou, Peng-Fei and Ran, Shi-Ju},
  year = {2024},
  month = may,
  number = {arXiv:2305.06058},
  eprint = {2305.06058},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.06058},
  urldate = {2025-05-13},
  abstract = {Neural network (NN) designed for challenging machine learning tasks is in general a highly nonlinear mapping that contains massive variational parameters. High complexity of NN, if unbounded or unconstrained, might unpredictably cause severe issues including over-fitting, loss of generalization power, and unbearable cost of hardware. In this work, we propose a general compression scheme that significantly reduces the variational parameters of NN by encoding them to deep automatically-differentiable tensor network (ADTN) that contains exponentially-fewer free parameters. Superior compression performance of our scheme is demonstrated on several widely-recognized NN's (FC-2, LeNet-5, AlextNet, ZFNet and VGG-16) and datasets (MNIST, CIFAR-10 and CIFAR-100). For instance, we compress two linear layers in VGG-16 with approximately \$10{\textasciicircum}\{7\}\$ parameters to two ADTN's with just 424 parameters, where the testing accuracy on CIFAR-10 is improved from \$90.17 {\textbackslash}\%\$ to \$91.74{\textbackslash}\%\$. Our work suggests TN as an exceptionally efficient mathematical structure for representing the variational parameters of NN's, which exhibits superior compressibility over the commonly-used matrices and multi-way arrays.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/liujinguo/Zotero/storage/WZF3QWXE/Qing et al. - 2024 - Compressing neural network by tensor network with exponentially fewer variational parameters.pdf;/Users/liujinguo/Zotero/storage/YPA6M4GD/2305.html}
}

@article{Haegeman2016,
  title = {Unifying Time Evolution and Optimization with Matrix Product States},
  author = {Haegeman, Jutho and Lubich, Christian and Oseledets, Ivan and Vandereycken, Bart and Verstraete, Frank},
  year = {2016},
  journal = {Physical Review B},
  eprint = {1408.5056},
  issn = {24699969},
  doi = {10.1103/PhysRevB.94.165116},
  abstract = {We show that the time-dependent variational principle provides a unifying framework for time-evolution methods and optimisation methods in the context of matrix product states. In particular, we introduce a new integration scheme for studying time-evolution, which can cope with arbitrary Hamiltonians, including those with long-range interactions. Rather than a Suzuki-Trotter splitting of the Hamiltonian, which is the idea behind the adaptive time-dependent density matrix renormalization group method or time-evolving block decimation, our method is based on splitting the projector onto the matrix product state tangent space as it appears in the Dirac-Frenkel time-dependent variational principle. We discuss how the resulting algorithm resembles the density matrix renormalization group (DMRG) algorithm for finding ground states so closely that it can be implemented by changing just a few lines of code and it inherits the same stability and efficiency. In particular, our method is compatible with any Hamiltonian for which DMRG can be implemented efficiently and DMRG is obtained as a special case of imaginary time evolution with infinite time step.},
  archiveprefix = {arXiv},
  file = {/Users/liujinguo/Zotero/storage/ECHHWZEI/Haegeman et al_2016_Unifying time evolution and optimization with matrix product states.pdf}
}
